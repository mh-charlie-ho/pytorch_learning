{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOshj1zQMIV"
      },
      "source": [
        "# CNN_Plant_Seedling_Classification\n",
        "\n",
        "\n",
        "此範例會使用模組化的軟體設計方法，它將一個大型的程式或專案拆分成更小、更容易管理的模塊或組件。每個模塊執行特定的功能或處理特定的任務，並且可以獨立開發、測試和維護。這樣的設計有助於提高程式碼的可讀性、可重用性和可維護性，並且讓不同團隊的開發者能夠協作更輕鬆。在PyTorch中，模組化通常表現為創建獨立的模型、函數和類別，每個模塊負責特定的任務，並且可以輕鬆組合在一起以構建更大的深度學習模型。\n",
        "\n",
        "本次課程將會把pytorch訓練流程分成以下幾個模組來教學:\n",
        "1. `Dataset`\n",
        "2. `Dataloader`\n",
        "3. `Model`\n",
        "4. `Train` Function\n",
        "5. `Valid` Function\n",
        "6. `Plot Curve` Function\n",
        "7. `Predict` Function\n",
        "8. `Main` Function\n",
        "9. `Addition` Customize model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOozzhScRUW0"
      },
      "source": [
        "## Download Datasets\n",
        "\n",
        "此範例使用 kaggle 上縮減過的 Plant Seedlings Classification資料集，請先至雲端 (https://drive.google.com/file/d/1QhirbRNJG_-XbiqIpxu5IWSspKGPAoyR/view?usp=sharing) 中下載`plant-seedlings-classification.zip`，透過`google.colab`套件，我們可以讓 Colab 上的程式直接讀取自己的雲端硬碟。\n",
        "\n",
        "\n",
        "執行下面的code之前，請先確保自己的google drive中已經有`plant-seedlings-classification.zip`檔案，並確保檔案位置正確。\n",
        "\n",
        "掛載自己的google drive後，雲端硬碟的根目錄為: `/content/gdrive/MyDrive/`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/MyDrive/plant_seedlings_classification/' # dataset's dir you want to unzip\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "  zip_dir = '/content/gdrive/MyDrive/plant-seedlings-classification.zip' # your zip file's dir\n",
        "\n",
        "  with zipfile.ZipFile(zip_dir, 'r') as zip_ref:\n",
        "      zip_ref.extractall(data_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "1O82OWk7PFCE",
        "outputId": "fdff0ff5-08c8-4b52-d148-414415ffe027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/plant-seedlings-classification.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bddc51a0c2ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mzip_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/plant-seedlings-classification.zip'\u001b[0m \u001b[0;31m# your zip file's dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/plant-seedlings-classification.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_t2vq4iuChqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Package"
      ],
      "metadata": {
        "id": "LSIrU_dAIIID"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4wB0uKSBaR2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms as tsfm\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from IPython import display\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config\n",
        "Config of Hyperparameter"
      ],
      "metadata": {
        "id": "0a-lDD09IPFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')"
      ],
      "metadata": {
        "id": "13gLt3SPISiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prY6qLtPRj55"
      },
      "source": [
        "## 1. Custom Pytorch Dataset\n",
        "\n",
        "\n",
        "A `Dataset` in Pytorch should have three methods:\n",
        "\n",
        "1. `__init__`: Read data & preprocess\n",
        "2. `__len__`: return a integer indicating the size of the dataset\n",
        "3. `__getitem__`: given an index `idx`, return the `idx`-th sample\n",
        "\n",
        "In our case, the download data is structured as:\n",
        "```\n",
        "├── train\n",
        "│   ├── Black-grass (Class name)\n",
        "│   │   ├── 0050f38b3.png\n",
        "│   │   ├── 0183fdf68.png\n",
        "│   │   ├── 0260cffa8.png\n",
        "│   │   ├── ...\n",
        "│   └── Charlock\n",
        "│   │   ├── 022179d65.png\n",
        "│   │   ├── 02c95e601.png\n",
        "│   │   ├── 04098447d.png\n",
        "│   │   ├── ...\n",
        "│   └── ...\n",
        "├── test\n",
        "│   ├── 0021e90e4.png\n",
        "│   ├── 003d61042.png\n",
        "│   ├── 007b3da8b.png\n",
        "│   ├── ...\n",
        "```\n",
        "\n",
        "We can simply find all the pngs and load them into memory when needed. In common practice, dataset will receive one or more `torchvision.transforms` which transform the png (loaded as `PIL.Image`) into pytorch tensor.\n",
        "\n",
        "`Dataloader` batchify the samples in dataset, i.e. builds mini-batch from the data return by dataset's `__getitem__` . We then iterate the `Dataloader` for training.\n",
        "\n",
        "Usually `Dataloader` is finite and will run of mini-batches when we have seen all samples in `Dataset` once. However, to write our code easily, we can create an infinite `Sampler` that can guide the batchification process in `Dataloader` and make `Dataloader` supply mini-batches infinitely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6wV3S8ovtlE"
      },
      "source": [
        "class Train_data(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.data = ImageFolder(root=root_dir, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx]\n",
        "        return img, label\n",
        "\n",
        "class Pred_data(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.img_paths = list(Path(root_dir).glob('*.png'))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx])\n",
        "        img = self.transform(img)\n",
        "        img = img.unsqueeze(0)\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize dataset item for debug\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "transform = tsfm.Compose([\n",
        "    tsfm.Resize((224, 224)),\n",
        "    tsfm.ToTensor(),\n",
        "])\n",
        "\n",
        "whole_set = Train_data(\n",
        "    root_dir=train_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_set = Pred_data(\n",
        "    root_dir=test_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "num_images_to_display = 5\n",
        "fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n",
        "\n",
        "for i, (img, label) in enumerate(whole_set):\n",
        "    axs[i].imshow(img.permute(1, 2, 0))\n",
        "    axs[i].set_title(f'Class: {label}')\n",
        "    axs[i].axis('off')\n",
        "\n",
        "    num_images_to_display -= 1\n",
        "    if num_images_to_display == 0:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "num_images_to_display = 5\n",
        "fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n",
        "for i, img in enumerate(test_set):\n",
        "    axs[i].imshow(img[0].permute(1, 2, 0))\n",
        "    axs[i].set_title(f'Test img: {i}')\n",
        "    axs[i].axis('off')\n",
        "\n",
        "    num_images_to_display -= 1\n",
        "    if num_images_to_display == 0:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tOGcORfOT2Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split train, valid set and Create Dataloader:\n"
      ],
      "metadata": {
        "id": "weRBnmJ2UxF-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDelZWMpDB9i"
      },
      "source": [
        "train_set, valid_set = random_split(whole_set, [0.8, 0.2])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrdV2X93Yh9E"
      },
      "source": [
        "## 3. Create Model\n",
        "\n",
        "A `nn.Module` in Pytorch should have two methods:\n",
        "\n",
        "1. `__init__`: Initialize your model & define layers\n",
        "2. `forward`: Compute output of your Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBv5LjNGDq8L"
      },
      "source": [
        "class resnet_50(nn.Module):\n",
        "    def __init__(self, num_classes = 12):\n",
        "        super(resnet_50, self).__init__()\n",
        "        # pytorch built-in models\n",
        "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        # set model layers trainable\n",
        "        for param in self.resnet50.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # redifine/customize last classification layer\n",
        "        self.resnet50.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet50(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test model for debug\n",
        "\n",
        "model = resnet_50(num_classes=12).cuda()\n",
        "# print(model)\n",
        "x = torch.rand(1, 3, 224, 224).cuda()\n",
        "y = model(x)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "cI969Z-K1Gn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkxgGKVwdyda"
      },
      "source": [
        "## 4. Define Train Function(for one epoch):\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_loader, epoch, total_epochs, batch_size):\n",
        "    model.train()\n",
        "    train_loss, train_acc = [], []\n",
        "\n",
        "    tqdm_iter = tqdm(train_loader, desc=\"Epoch: {}/{} ({}%) | Training loss: NaN\".format(\n",
        "    epoch, total_epochs, int(epoch/total_epochs * 100)), leave=False)\n",
        "    epoch_loss, epoch_acc = 0.0, 0.0\n",
        "    for batch_idx, (data, label) in enumerate(tqdm_iter):\n",
        "        data, target = data.cuda(), label.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc = (output.argmax(dim=1) == target).float().mean().item()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "\n",
        "        tqdm_iter.set_description(\"Epoch: {}/{} ({}%) | Training loss: {:.6f} | Training Acc: {:.6f}\".format(\n",
        "        epoch + 1, total_epochs, int((epoch+1)/total_epochs * 100), round(loss.item(), 6), round(acc, 6)))\n",
        "\n",
        "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
      ],
      "metadata": {
        "id": "wmY5HWvtqIXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug \"train\" function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "loss, acc = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    train_loader,\n",
        "    epoch=1,\n",
        "    total_epochs=1,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(loss, acc)"
      ],
      "metadata": {
        "id": "13uX3REdWr4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define Valid Function(for one epoch):\n"
      ],
      "metadata": {
        "id": "RiL9Kb0PKvwl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V29iauDlu4t5"
      },
      "source": [
        "def valid(model, criterion, valid_loader, epoch, total_epochs, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    tqdm_iter = tqdm(valid_loader, desc=\"Epoch: {}/{} ({}%) | Valid loss: NaN\".format(\n",
        "    epoch, total_epochs, int(epoch/total_epochs * 100)), leave=False)\n",
        "    epoch_loss, epoch_acc = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(tqdm_iter):\n",
        "            data, target = data.cuda(), label.cuda()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            acc = (output.argmax(dim=1) == target).float().mean().item()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc\n",
        "\n",
        "            tqdm_iter.set_description(\"Epoch: {}/{} ({}%) | Valid loss: {:.6f} | Valid Acc: {:.6f}\".format(\n",
        "            epoch + 1, total_epochs, int((epoch+1)/total_epochs * 100), round(loss.item(), 6), round(acc, 6)))\n",
        "\n",
        "    return epoch_loss / len(valid_loader), epoch_acc / len(valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug \"valid\" function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss, acc = valid(\n",
        "    model,\n",
        "    criterion,\n",
        "    valid_loader,\n",
        "    epoch=1,\n",
        "    total_epochs=1,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(loss, acc)"
      ],
      "metadata": {
        "id": "aFmHlyWtW5tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Plot Learning Curve Function:"
      ],
      "metadata": {
        "id": "lFRQ5iuQQydh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXNZ6owLloYe"
      },
      "source": [
        "def Plot(title, ylabel, epochs, train_loss, valid_loss):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.plot(epochs, train_loss)\n",
        "    plt.plot(epochs, valid_loss)\n",
        "    plt.legend(['train', 'valid'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug \"Plot\" function\n",
        "debug_epochs = [1, 2, 3, 4, 5]\n",
        "debug_train_loss = [0.1, 0.08, 0.06, 0.05, 0.04]\n",
        "debug_valid_loss = [0.2, 0.15, 0.12, 0.1, 0.09]\n",
        "\n",
        "Plot(\"Training and Validation Loss\", 'Loss', debug_epochs, debug_train_loss, debug_valid_loss)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bOjwmmscaHtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Predict Function:"
      ],
      "metadata": {
        "id": "zFPmxmdOZLu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(loader, model):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for data in tqdm(loader):\n",
        "        pred = model(data.cuda())\n",
        "        cls = torch.argmax(pred, dim=1)\n",
        "        preds.append(cls)\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "qNpjt-afZLOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Predict result\n",
        "def view_pred_result(preds, num_images_to_display=5):\n",
        "    labels = ['Black-grass', 'Charlock' , 'Cleavers' , 'Common Chickweed' , 'Common wheat' , 'Fat Hen' , 'Loose Silky-bent' , 'Maize' , 'Scentless Mayweed' , 'Shepherds Purse', 'Small-flowered Cranesbill' , 'Sugar beet']\n",
        "    fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n",
        "    for i, img in enumerate(test_set):\n",
        "        axs[i].imshow(img[0].permute(1, 2, 0))\n",
        "        axs[i].set_title(labels[preds[i].item()])\n",
        "        axs[i].axis('off')\n",
        "\n",
        "        num_images_to_display -= 1\n",
        "        if num_images_to_display == 0:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OIZ5WD5pvBRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug \"Predict\" function & \"View_Predict_result\" function\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "transform = tsfm.Compose([\n",
        "    tsfm.Resize((224, 224)),\n",
        "    tsfm.ToTensor(),\n",
        "])\n",
        "test_set = Pred_data(\n",
        "    root_dir=test_dir,\n",
        "    transform=transform\n",
        ")\n",
        "model = resnet_50(num_classes=12).cuda()\n",
        "\n",
        "preds = predict(test_set, model)\n",
        "view_pred_result(preds)"
      ],
      "metadata": {
        "id": "_c-diUny6Vz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Main Function(training pipeline):\n",
        "1. `Set Hyperparameters`: `batct_size`, `learning rate`, `epochs`...\n",
        "2. `Initial`: initial `dataset`, `dataloader`, `model`\n",
        "3. `Train`: Do train\n",
        "4. `Valid`: Do valid\n",
        "5. repeat `3.`  `4.` epochs times\n",
        "6. `Plot curve`: Plot learning curve to observe the learning progress\n",
        "7. `Predict`: Use the trained model to predict the results of the test set"
      ],
      "metadata": {
        "id": "rDIbCKKe89C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # initial transform\n",
        "    transform = tsfm.Compose([\n",
        "        tsfm.Resize((224, 224)),\n",
        "        tsfm.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # initial dataset\n",
        "    whole_set = Train_data(\n",
        "        root_dir=train_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_set = Pred_data(\n",
        "        root_dir=test_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # split train valid and initial dataloader\n",
        "    train_set_size = int(len(whole_set) * 0.8)\n",
        "    valid_set_size = len(whole_set) - train_set_size\n",
        "    train_set, valid_set = random_split(whole_set, [train_set_size, valid_set_size])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
        "\n",
        "    # initial model\n",
        "    model = VGG16(num_classes=12).cuda()\n",
        "\n",
        "    # initial loss_function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # initial plot values\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "    epoch_list = []\n",
        "\n",
        "    # repeat train and valid epochs times\n",
        "    print(epochs)\n",
        "    for epoch in range(epochs):\n",
        "      epoch_list.append(epoch + 1)\n",
        "\n",
        "      loss, acc = train(\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          epoch=epoch,\n",
        "          total_epochs=epochs,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "      train_loss.append(loss)\n",
        "      train_acc.append(acc)\n",
        "      print(f'Avg train Loss: {loss}, Avg train acc: {acc}')\n",
        "\n",
        "      loss, acc = valid(\n",
        "          model,\n",
        "          criterion,\n",
        "          valid_loader,\n",
        "          epoch=epoch,\n",
        "          total_epochs=epochs,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "      valid_loss.append(loss)\n",
        "      valid_acc.append(acc)\n",
        "      print(f'Avg valid Loss: {loss}, Avg valid acc: {acc}')\n",
        "\n",
        "    Plot(\"Loss Curve\", 'Loss', epoch_list, train_loss, valid_loss)\n",
        "    Plot(\"Accuarcy Curve\", 'Acc', epoch_list, train_acc, valid_acc)\n",
        "\n",
        "    preds = predict(test_set, model)\n",
        "    view_pred_result(preds)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "za9NH6rY1x-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Addition: Customize your own model\n",
        "Create your own deep learning model, by define the inner layers in hand-craft<br>\n",
        "Example for VGG16 model: https://arxiv.org/abs/1409.1556"
      ],
      "metadata": {
        "id": "ZBzQKwt9F0i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(VGG16, self).__init__()\n",
        "        # input layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        #  classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "wKuXQ15OGKLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model to debug\n",
        "x = torch.rand(1, 3, 224, 224)\n",
        "model = VGG16(num_classes=12)\n",
        "y = model(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "kE23DmwHGl9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun training for VGG16 model"
      ],
      "metadata": {
        "id": "ERXYTM7ORcp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # initial transform\n",
        "    transform = tsfm.Compose([\n",
        "        tsfm.Resize((224, 224)),\n",
        "        tsfm.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # initial dataset\n",
        "    whole_set = Train_data(\n",
        "        root_dir=train_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_set = Pred_data(\n",
        "        root_dir=test_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # split train valid and initial dataloader\n",
        "    train_set_size = int(len(whole_set) * 0.8)\n",
        "    valid_set_size = len(whole_set) - train_set_size\n",
        "    train_set, valid_set = random_split(whole_set, [train_set_size, valid_set_size])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
        "\n",
        "    # initial model\n",
        "    model = VGG16(num_classes=12).cuda()\n",
        "\n",
        "    # initial loss_function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # initial plot values\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "    epoch_list = []\n",
        "\n",
        "    # repeat train and valid epochs times\n",
        "    print(epochs)\n",
        "    for epoch in range(epochs):\n",
        "      epoch_list.append(epoch + 1)\n",
        "\n",
        "      loss, acc = train(\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          epoch=epoch,\n",
        "          total_epochs=epochs,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "      train_loss.append(loss)\n",
        "      train_acc.append(acc)\n",
        "      print(f'Avg train Loss: {loss}, Avg train acc: {acc}')\n",
        "\n",
        "      loss, acc = valid(\n",
        "          model,\n",
        "          criterion,\n",
        "          valid_loader,\n",
        "          epoch=epoch,\n",
        "          total_epochs=epochs,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "      valid_loss.append(loss)\n",
        "      valid_acc.append(acc)\n",
        "      print(f'Avg valid Loss: {loss}, Avg valid acc: {acc}')\n",
        "\n",
        "    Plot(\"Loss Curve\", 'Loss', epoch_list, train_loss, valid_loss)\n",
        "    Plot(\"Accuarcy Curve\", 'Acc', epoch_list, train_acc, valid_acc)\n",
        "\n",
        "    preds = predict(test_set, model)\n",
        "    view_pred_result(preds)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "C2brZ16wRjMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}